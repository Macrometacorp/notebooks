{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f63d4ffe",
   "metadata": {
    "kernelspec": {
     "display_name": "Javascript",
     "name": "javascript"
    }
   },
   "source": [
    "# Py09 - CI/CD Tutorial\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial we will build a simple CI/CD script for GDN plateform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13fcd6-1552-42de-adc2-40adc794baa9",
   "metadata": {},
   "source": [
    "## Pre-requisite\n",
    "\n",
    "Let's assume your \n",
    "\n",
    "- tenant name is an email address\n",
    "- user password is xxxxx.\n",
    "\n",
    "if you need to install pyc8, you can run the cell below, otherwise you may skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636b5920-c6c0-466c-b45f-742adfd14577",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyC8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4743f",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries & Define Variables\n",
    "\n",
    "The first step is to import the libraries we need and define the variables we will be using in this tutorial. This is also the right place to add your GDN login credentials. i.e. your email and password. You will also need to make sure you have specified the correct federation URL. In this example it is \"gdn.paas.macrometa.io\" and we are using the default geo fabric \"_system\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58664212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "from c8 import C8Client\n",
    "from c8.request import Request\n",
    "import random\n",
    "import six\n",
    "import base64\n",
    "import time\n",
    "import threading\n",
    "\n",
    "# Variables\n",
    "global_url = \"gdn.paas.macrometa.io\"\n",
    "email = \"email\"  # <-- Email goes here\n",
    "password = \"password\"  # <-- password goes here\n",
    "geo_fabric = \"_system\"\n",
    "\n",
    "COLLECTION_TYPE = {\n",
    "    \"DOCUMENT\": \"document\",\n",
    "    \"KV\": \"graph\",\n",
    "}\n",
    "\n",
    "fabric_list = [\"cicd-fabric\"]\n",
    "\n",
    "collection_list = [\n",
    "    {\n",
    "        \"name\": \"Doc1\",\n",
    "        \"type\": COLLECTION_TYPE[\"DOCUMENT\"],\n",
    "        \"isEdge\": False,\n",
    "        \"hasStream\": True,\n",
    "        \"noOfInsertOperation\": 100,\n",
    "        \"noOfUpdateOperation\": 1,\n",
    "        \"noOfDeleteOperations\": 100\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Doc2\",\n",
    "        \"type\": COLLECTION_TYPE[\"DOCUMENT\"],\n",
    "        \"isEdge\": False,\n",
    "        \"hasStream\": True,\n",
    "        \"noOfInsertOperation\": 100,\n",
    "        \"noOfUpdateOperation\": 1,\n",
    "        \"noOfDeleteOperations\": 100\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"KV1\",\n",
    "        \"type\": COLLECTION_TYPE[\"KV\"],\n",
    "        \"hasStream\": True\n",
    "    }\n",
    "]\n",
    "\n",
    "stream_list = [\n",
    "    {\n",
    "        \"name\": \"gstream\",\n",
    "        \"isLocal\": False,\n",
    "        \"noOfInsertOperation\": 10,\n",
    "        \"noOfUpdateOperation\": 0,\n",
    "        \"noOfDeleteOperations\": 0\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"lstream\",\n",
    "        \"isLocal\": True,\n",
    "        \"noOfInsertOperation\": 10,\n",
    "        \"noOfUpdateOperation\": 0,\n",
    "        \"noOfDeleteOperations\": 0\n",
    "    }\n",
    "]\n",
    "\n",
    "query_worker_list = {\n",
    "    \"insert_data\": {\n",
    "        \"name\": \"insertRecord\",\n",
    "        \"value\": \"\"\"FOR i IN 1..100\n",
    "              INSERT {\n",
    "                    \"firstname\":CONCAT(\"Halie\", TO_STRING(i)),\n",
    "                    \"lastname\":CONCAT(\"Linkie\", TO_STRING(i)),\n",
    "                    \"email\":CONCAT(\"hlinkie0\", TO_STRING(i),\"@irs.gov\"),\n",
    "                    \"zipcode\": CONCAT(\"2950-53\", TO_STRING(i))\n",
    "                } INTO COLLECTION_NAME\"\"\",\n",
    "        \"parameter\": {},\n",
    "    },\n",
    "    \"get_data\": {\n",
    "        \"name\": \"getRecords\",\n",
    "        \"value\": \"FOR doc IN COLLECTION_NAME RETURN doc\",\n",
    "    },\n",
    "    \"update_data\": {\n",
    "        \"name\": \"updateRecord\",\n",
    "        \"value\": \"\"\"FOR doc IN COLLECTION_NAME \n",
    "      filter doc.email == 'hlinkie05@irs.gov'\n",
    "      UPDATE { _key:doc._key,  \\\"lastname\\\": \\\"cena\\\" }\n",
    "        IN COLLECTION_NAME\"\"\",\n",
    "    },\n",
    "    \"get_count\": {\n",
    "        \"name\": \"countRecords\",\n",
    "        \"value\": \"RETURN COUNT(FOR doc IN COLLECTION_NAME RETURN 1)\",\n",
    "    },\n",
    "    \"delete_data\": {\n",
    "        \"name\": \"deleteRecord\",\n",
    "        \"value\": \"\"\"FOR doc IN COLLECTION_NAME \n",
    "          filter doc.email == 'hlinkie03@irs.gov'\n",
    "          REMOVE doc \n",
    "          IN COLLECTION_NAME \"\"\",\n",
    "    },\n",
    "    \"delete_all_data\": {\n",
    "        \"name\": \"deleteAllRecord\",\n",
    "        \"value\": \"\"\"FOR doc IN COLLECTION_NAME\n",
    "        REMOVE doc \n",
    "        IN COLLECTION_NAME \"\"\",\n",
    "    },\n",
    "}\n",
    "\n",
    "graph_list = [\n",
    "    {\n",
    "        \"name\": \"social\",\n",
    "        \"edgeDefinitions\": {\n",
    "            \"collection\": \"relation\",\n",
    "            \"from\": [\"female\", \"male\"],\n",
    "            \"to\": [\"female\", \"male\"],\n",
    "        },\n",
    "    },\n",
    "    {\"name\": \"children\"},\n",
    "]\n",
    "\n",
    "stream_worker_list = [\n",
    "    {\n",
    "        \"name\": \"MockHeartRateDataGenerator\",\n",
    "        \"definition\": '''\n",
    "@App:name(\"MockHeartRateDataGenerator\")\n",
    "@App:qlVersion(\"2\")\n",
    "\n",
    "CREATE TRIGGER HeartRateDataGeneratorTrigger WITH ( interval = 10 sec );\n",
    "\n",
    "CREATE TABLE HeartRates (name string, bpm int);\n",
    "\n",
    "\n",
    "-- Note: Generating random bpm and name \n",
    "@info(name = 'ConsumeProcessedData')\n",
    "INSERT INTO HeartRates\n",
    "SELECT \n",
    "js:eval(\"['Vasili', 'Rivalee', 'Betty', 'Jennifer', 'Alane', 'Sarena', 'Bruno', 'Carolee', 'Emmott', 'Andre'][Math.floor(Math.random() * 10)]\",\"string\") as name,\n",
    "js:eval(\"Math.floor(Math.random() * 40) + 40\",\"int\") as bpm\n",
    "FROM HeartRateDataGeneratorTrigger;\n",
    "''',\n",
    "    },\n",
    "]\n",
    "\n",
    "web_socket_data = {}\n",
    "\n",
    "web_socket_list = []\n",
    "\n",
    "active_fabric = \"\"\n",
    "\n",
    "DOCUMENT_OPERATIONS = {\n",
    "    \"UPDATE\": \"UPDATE\",\n",
    "    \"INSERT\": \"INSERT\",\n",
    "    \"DELETE\": \"DELETE\",\n",
    "}\n",
    "\n",
    "info = {\n",
    "    \"noOfCollection\": 0,\n",
    "    \"noOfStreamWorker\": 0,\n",
    "    \"noOfQueryWorker\": 0,\n",
    "    \"noOfStream\": 0,\n",
    "    \"noOfFabric\": 0,\n",
    "    \"noOfGraph\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba931134",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Connecting to GDN\n",
    "\n",
    "Now that we have imported the required libraries and added our login details, we can connect to GDN. Do this by running the cell bellow.\n",
    "\n",
    "You will see the cell output reflect a successful connection. If not go back to the first step and check the details you entered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c4fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ------- CONNECTION SETUP  ------\")\n",
    "print(\"tenant: {}, geofabric:{}\".format(email, geo_fabric))\n",
    "client = C8Client(protocol='https', host=global_url, port=443,\n",
    "                  email=email, password=password,\n",
    "                  geofabric=geo_fabric)\n",
    "\n",
    "tenant = client.tenant(email=email, password=password)\n",
    "current_fabric = tenant.useFabric(geo_fabric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f00af",
   "metadata": {},
   "source": [
    "## 3. Creating Helper Functions\n",
    "\n",
    "Helper functions are basic building blocks for the CI/CD pipeline. These functions are responsible for creating/deleting collection, streams, query worker, stream worker, etc. based on the list provied in the section 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376438c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Logger function\n",
    "def log(message):\n",
    "  print(\"[{date}]: {message}\".format(\n",
    "      date=datetime.datetime.now(), message=message))\n",
    "\n",
    "# Generate the random numbers\n",
    "def get_random_number(min, max):\n",
    "  if min == max:\n",
    "    return min\n",
    "  return random.randint(min, max-1)\n",
    "\n",
    "# Create the document collection\n",
    "def create_document_collection(name, has_stream=True, is_edge=False):\n",
    "  has_collection = current_fabric.has_collection(name)\n",
    "  if has_collection:\n",
    "    return\n",
    "  current_fabric.create_collection(name=name, edge=is_edge)\n",
    "  if has_stream:\n",
    "    request = Request(\n",
    "        method='put',\n",
    "        endpoint='/collection/{}/stream'.format(name),\n",
    "        data={\n",
    "            \"hasStream\": True\n",
    "        }\n",
    "    )\n",
    "\n",
    "  def response_handler(resp):\n",
    "    if not resp.is_success:\n",
    "      raise ValueError(\"Failed to update stream flag {}\".format(name))\n",
    "  # In upcoming release this API call will be replaced by pyc8 internal function\n",
    "  current_fabric._execute(request, response_handler)\n",
    "  info[\"noOfCollection\"] += 1\n",
    "\n",
    "\n",
    "# Create the kv collection\n",
    "def create_kv_collection(name, has_stream=True):\n",
    "  has_collection = current_fabric.key_value.has_collection(name)\n",
    "  if has_collection:\n",
    "    return\n",
    "  current_fabric.key_value.create_collection(name, {\"stream\": has_stream})\n",
    "  info[\"noOfCollection\"] += 1\n",
    "\n",
    "# Create the query workers\n",
    "def create_query_worker(query_worker, query_worker_list):\n",
    "  try:\n",
    "    for collection in collection_list:\n",
    "      query_worker_name = \"{collectionName}_{queryWorkerName}\".format(\n",
    "          collectionName=collection[\"name\"], queryWorkerName=query_worker[\"name\"])\n",
    "      if collection[\"type\"] != COLLECTION_TYPE[\"DOCUMENT\"] or query_worker_name in query_worker_list:\n",
    "        continue\n",
    "      else:\n",
    "        data = {\n",
    "            \"query\": {\n",
    "                \"parameter\": query_worker.get(\"parameter\", {}),\n",
    "                \"name\": query_worker_name,\n",
    "                \"value\": query_worker[\"value\"].replace(\"COLLECTION_NAME\", collection[\"name\"])\n",
    "            }\n",
    "        }\n",
    "        current_fabric.save_restql(data)\n",
    "  except Exception as error:\n",
    "    log(\"Error while creating query worker {queryWorker} : -- {err}\".format(\n",
    "        queryWorker=query_worker[\"name\"], err=error))\n",
    "\n",
    "# Create the GeoFabric\n",
    "def create_fabric(name, fabric_list, local_dc_name, all_dc_list):\n",
    "  global_dc_list = set([] if len(all_dc_list) == 0 else\n",
    "                       [all_dc_list[get_random_number(0, len(all_dc_list) - 1)],\n",
    "                        all_dc_list[get_random_number(0, len(all_dc_list) - 1)], ]\n",
    "                       )\n",
    "  global_dc_list.add(local_dc_name)\n",
    "  if name in fabric_list:\n",
    "    return\n",
    "  current_fabric.create_fabric(name, dclist=global_dc_list)\n",
    "  info[\"noOfFabric\"] += 1\n",
    "\n",
    "# Create the graph\n",
    "def crate_graph(name, details=None):\n",
    "  has_graph = current_fabric.has_graph(name)\n",
    "  if has_graph:\n",
    "    return\n",
    "  graph = current_fabric.create_graph(name)\n",
    "  if details is not None:\n",
    "    graph.create_edge_definition(\n",
    "        edge_collection=details[\"collection\"],\n",
    "        from_vertex_collections=details[\"from\"],\n",
    "        to_vertex_collections=details[\"to\"]\n",
    "    )\n",
    "  info[\"noOfGraph\"] += 1\n",
    "\n",
    "# Create the stream\n",
    "def create_stream(name, is_local):\n",
    "  is_stream_exist = current_fabric.has_stream(name, local=is_local)\n",
    "  if is_stream_exist:\n",
    "    return\n",
    "  current_fabric.create_stream(name, is_local)\n",
    "  info[\"noOfStream\"] += 1\n",
    "\n",
    "\n",
    "def websocket_listener(name, subscriber):\n",
    "  global web_socket_data\n",
    "  \n",
    "  web_socket_data[name] = {}\n",
    "  web_socket_data[name][DOCUMENT_OPERATIONS[\"UPDATE\"]] = []\n",
    "  web_socket_data[name][DOCUMENT_OPERATIONS[\"INSERT\"]] = []\n",
    "  web_socket_data[name][DOCUMENT_OPERATIONS[\"DELETE\"]] = []\n",
    "    \n",
    "  try:\n",
    "      while subscriber.connected:\n",
    "        #Listen on stream for any receiving msg's\n",
    "        message = json.loads(subscriber.recv())\n",
    "        payload = json.loads(base64.b64decode(message[\"payload\"]))\n",
    "        operationType = message[\"properties\"].get(\n",
    "            \"op\", DOCUMENT_OPERATIONS[\"INSERT\"])\n",
    "        if operationType == DOCUMENT_OPERATIONS[\"UPDATE\"]:\n",
    "          web_socket_data[name][DOCUMENT_OPERATIONS[\"UPDATE\"]].append(payload)\n",
    "        elif operationType == DOCUMENT_OPERATIONS[\"DELETE\"]:\n",
    "          web_socket_data[name][DOCUMENT_OPERATIONS[\"DELETE\"]].append(payload)\n",
    "        else:\n",
    "          web_socket_data[name][DOCUMENT_OPERATIONS[\"INSERT\"]].append(payload)\n",
    "        #Acknowledge the received msg.\n",
    "        subscriber.send(json.dumps({'messageId': message['messageId']}))\n",
    "  except Exception as e:\n",
    "    log(\"WebSocket Closed {}\".format(name))\n",
    "\n",
    "#  Create the stream subscriber\n",
    "def create_subscriber(\n",
    "    name,\n",
    "    is_local=True,\n",
    "    is_collection_stream=False\n",
    "):\n",
    "  global web_socket_list\n",
    "  \n",
    "  try:\n",
    "    subscriber = current_fabric.stream().subscribe(name, subscription_name=name,\n",
    "                                                   local=is_local, isCollectionStream=is_collection_stream)\n",
    "    web_socket_list.append(subscriber)\n",
    "    x = threading.Thread(target=websocket_listener, args=(name, subscriber,))\n",
    "    x.start()\n",
    "\n",
    "  except Exception as error:\n",
    "    error.message = \\\n",
    "        \"Error while creating subscriber {} : -- {}\".format(name, error)\n",
    "    raise error\n",
    "\n",
    "# Create the stream publisher\n",
    "def create_publisher(name, is_local):\n",
    "  global web_socket_list\n",
    "  producer = current_fabric.stream().create_producer(\n",
    "      name,\n",
    "      local=is_local\n",
    "  )\n",
    "  web_socket_list.append(producer)\n",
    "  for i in range(10):\n",
    "    message = {\n",
    "        \"firstname\": \"Halie{}\".format(i),\n",
    "        \"lastname\": \"Linkie{}\".format(i),\n",
    "        \"email\": \"hlinkie0{}@irs.gov\".format(i),\n",
    "        \"zipcode\": \"2950-53{}\".format(i),\n",
    "    }\n",
    "    payload = {\n",
    "        \"payload\": base64.b64encode(six.b(json.dumps(message))).decode(\"utf-8\")\n",
    "    }\n",
    "    producer.send(json.dumps(payload))\n",
    "\n",
    "# Create the stream based on provided stream list in stream_list\n",
    "def create_streams():\n",
    "  for stream in stream_list:\n",
    "    create_stream(stream[\"name\"], stream[\"isLocal\"])\n",
    "\n",
    "# Create the stream subscribers based on provided stream list in stream_list\n",
    "def create_stream_subscribers():\n",
    "  for stream in stream_list:\n",
    "    create_subscriber(stream[\"name\"], stream[\"isLocal\"])\n",
    "\n",
    "# Create the stream publisher based on provided stream list in stream_list\n",
    "def create_stream_publisher():\n",
    "  for stream in stream_list:\n",
    "    create_publisher(stream[\"name\"], stream[\"isLocal\"])\n",
    "\n",
    "# Create the new fabric\n",
    "def create_new_fabrics():\n",
    "  global active_fabric\n",
    "  local_dc = client.get_local_dc()\n",
    "  local_dc_name = local_dc[\"_key\"]\n",
    "  all_dc_list = client.get_dc_list()\n",
    "  global_dc_list = list(filter(\n",
    "      lambda dc_name: dc_name != local_dc_name,\n",
    "      all_dc_list))\n",
    "  existing_fabric_list = current_fabric.fabrics()\n",
    "  for fabric in fabric_list:\n",
    "    create_fabric(fabric, existing_fabric_list, local_dc_name, global_dc_list)\n",
    "  active_fabric = fabric_list[0 if len(\n",
    "      fabric_list) == 1 else get_random_number(0, len(fabric_list) - 1)]\n",
    "  use_fabric()\n",
    "\n",
    "# Create the collections based on provided collection list in collection_list\n",
    "def create_collections():\n",
    "  for collection in collection_list:\n",
    "    if collection[\"type\"] == COLLECTION_TYPE[\"KV\"]:\n",
    "      create_kv_collection(collection[\"name\"], collection[\"hasStream\"])\n",
    "    else:\n",
    "      create_document_collection(\n",
    "          collection[\"name\"], collection[\"hasStream\"], collection[\"isEdge\"])\n",
    "\n",
    "# Create the query workers based on provided query worker list in query_worker_list\n",
    "def create_query_workers():\n",
    "  for key, value in query_worker_list.items():\n",
    "    existing_query_workers = current_fabric.get_all_restql()\n",
    "    existing_query_workers = list(\n",
    "        map(lambda query_worker: query_worker[\"name\"], existing_query_workers))\n",
    "    create_query_worker(value, existing_query_workers)\n",
    "    info[\"noOfQueryWorker\"] += 1\n",
    "\n",
    "# Create the subscriber based on provided collection list in collection_list\n",
    "def create_collections_subscribers():\n",
    "  _collectionList = list(filter(\n",
    "      lambda collection: collection[\"type\"] == COLLECTION_TYPE[\"DOCUMENT\"], collection_list))\n",
    "  for collection in _collectionList:\n",
    "    create_subscriber(collection[\"name\"], is_collection_stream=True)\n",
    "\n",
    "# Create the graphs based on provided graph list in graph_list\n",
    "def create_graphs():\n",
    "  for graph in graph_list:\n",
    "    crate_graph(graph[\"name\"], graph.get(\"edgeDefinitions\", None))\n",
    "\n",
    "# Create the stream worker based on provided stream list in streamAppList\n",
    "def create_stream_workers():\n",
    "  stream_app_list = current_fabric.retrive_stream_app()\n",
    "  stream_app_list = list(\n",
    "      map(lambda stream_app: stream_app[\"name\"], stream_app_list[\"streamApps\"]))\n",
    "  for worker in stream_worker_list:\n",
    "    if worker[\"name\"] not in stream_app_list:\n",
    "        result = current_fabric.create_stream_app(worker[\"definition\"])\n",
    "        info[\"noOfStreamWorker\"] += 1\n",
    "\n",
    "# Activate the stream worker based on provided stream list in stream_worker_list\n",
    "def activate_stream_workers(is_active=True):\n",
    "  for worker in stream_worker_list:\n",
    "    current_fabric.stream_app(worker[\"name\"]).change_state(is_active)\n",
    "\n",
    "# set current fabric to jsc8 client\n",
    "def use_fabric():\n",
    "  global current_fabric\n",
    "  current_fabric = tenant.useFabric(active_fabric)\n",
    "  log(\"Using fabric {}\".format(active_fabric))\n",
    "\n",
    "# Execute all the query workers provided in the query_worker_list\n",
    "# for every document collection present in collection_list\n",
    "def execute_query_workers():\n",
    "  for query, value in query_worker_list.items():\n",
    "    for collection in collection_list:\n",
    "      if collection[\"type\"] == COLLECTION_TYPE[\"DOCUMENT\"]:\n",
    "        current_fabric.execute_restql(\n",
    "            \"{collectionName}_{queryWorkerName}\".format(\n",
    "                collectionName=collection[\"name\"], queryWorkerName=value[\"name\"]),\n",
    "            value.get(\"data\", {}))\n",
    "\n",
    "# Read the websocket data and verify the number of operations\n",
    "def verify_streams():\n",
    "  # Waiting for 5 seconds so subscriber receives all the data\n",
    "  time.sleep(5)\n",
    "  for stream in stream_list:\n",
    "    wsData = web_socket_data.get(stream[\"name\"], None)\n",
    "    if not (wsData and len(wsData[DOCUMENT_OPERATIONS[\"INSERT\"]]) == stream[\"noOfInsertOperation\"] and\n",
    "            len(wsData[DOCUMENT_OPERATIONS[\"UPDATE\"]]) == stream[\"noOfUpdateOperation\"] and\n",
    "            len(wsData[DOCUMENT_OPERATIONS[\"DELETE\"]]) == stream[\"noOfDeleteOperations\"]):\n",
    "      log(\"Websocket data validation failed for {streamName}\".format(\n",
    "          streamName=stream[\"name\"]))\n",
    "\n",
    "# Validate the collection stream websocket data and operations\n",
    "def verify_collection_stream():\n",
    "  # Waiting for 5 seconds so subscriber receives all the data\n",
    "  time.sleep(5)\n",
    "  tmpCol = list(filter(\n",
    "      lambda collection: collection[\"type\"] == COLLECTION_TYPE[\"DOCUMENT\"], collection_list))\n",
    "  for collection in tmpCol:\n",
    "    wsData = web_socket_data[collection[\"name\"]]\n",
    "    if not (wsData and\n",
    "            len(wsData[DOCUMENT_OPERATIONS[\"INSERT\"]]) == collection[\"noOfInsertOperation\"] and\n",
    "            len(wsData[DOCUMENT_OPERATIONS[\"UPDATE\"]]) == collection[\"noOfUpdateOperation\"] and\n",
    "            len(wsData[DOCUMENT_OPERATIONS[\"DELETE\"]]) == collection[\"noOfDeleteOperations\"]):\n",
    "      log(\"Websocket data validation failed for collection {collectionName}\".format(\n",
    "          collectionName=collection[\"name\"]))\n",
    "\n",
    "# Delete all the query workers present in the fabric\n",
    "def delete_query_workers():\n",
    "  query_worker_list = current_fabric.get_all_restql()\n",
    "  for query in query_worker_list:\n",
    "    current_fabric.delete_restql(query[\"name\"])\n",
    "\n",
    "# Delete all the graphs present in the fabric\n",
    "def delete_graphs():\n",
    "  graph_list = current_fabric.graphs()\n",
    "  for graph in graph_list:\n",
    "    current_fabric.delete_graph(name=graph[\"name\"], drop_collections=True)\n",
    "\n",
    "# Delete all the collection present in the fabric\n",
    "def delete_collections():\n",
    "  collection_list = current_fabric.collections()\n",
    "  for collection in list(filter(\n",
    "          lambda collection: not collection[\"system\"],\n",
    "          collection_list)):\n",
    "    current_fabric.delete_collection(collection[\"name\"])\n",
    "\n",
    "# Delete all the fabric workers present in the fabric_list\n",
    "def delete_fabrics():\n",
    "  for fabric in fabric_list:\n",
    "    current_fabric.delete_fabric(fabric)\n",
    "\n",
    "# Terminate all the websocket\n",
    "def clear_web_socket():\n",
    "  for web_socket in web_socket_list:\n",
    "    web_socket.close()\n",
    "\n",
    "# Delete all the streams present in the fabric\n",
    "def delete_streams():\n",
    "  streams_list = []\n",
    "  global_stream = current_fabric.streams()\n",
    "  local_stream = current_fabric.streams(True)\n",
    "  streams_list = global_stream + local_stream\n",
    "\n",
    "  def response_handler(resp):\n",
    "    if not resp.is_success:\n",
    "      raise ValueError(\"Failed to delete stream: {}\")\n",
    "  for stream in streams_list:\n",
    "    request = Request(\n",
    "        method='delete',\n",
    "        endpoint='/streams/{}'.format(stream[\"name\"])\n",
    "    )\n",
    "    # In upcoming release this API call will be replaced by pyc8 internal function\n",
    "    current_fabric._execute(request, response_handler)\n",
    "\n",
    "# Delete all the streams worker present in the fabric\n",
    "def delete_stream_worker():\n",
    "  all_steam_apps = current_fabric.retrive_stream_app()\n",
    "  for stream_app in all_steam_apps[\"streamApps\"]:\n",
    "    current_fabric.stream_app(stream_app[\"name\"]).delete()\n",
    "\n",
    "# It will delete the query-workers, graphs, collections, streams, stream-workers and fabric\n",
    "def clear():\n",
    "  global active_fabric\n",
    "  use_fabric()\n",
    "  delete_query_workers()\n",
    "  delete_graphs()\n",
    "  delete_collections()\n",
    "  delete_streams()\n",
    "  delete_stream_worker()\n",
    "  active_fabric = \"_system\"\n",
    "  use_fabric()\n",
    "  delete_fabrics()\n",
    "  clear_web_socket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febace29-21e0-4d75-993a-198f546a5737",
   "metadata": {},
   "source": [
    "## 4. Create CI/CD Pipeline\n",
    "\n",
    "Below function contains the list of sub functions.Each subfunction is responsible for either creating or deleting the elements in the GDN. \n",
    "\n",
    "Current flow is as below \n",
    "- Create fabrics\n",
    "- Pick any random fabric from created fabric\n",
    "- Create collections\n",
    "- Create query worker for each collection \n",
    "- Create stream workers \n",
    "- Activate the stream workers\n",
    "- Create the collection subscribers\n",
    "- Execute the query workers\n",
    "- Execute the CRUD using import API and truncate the collection. This operation is operformed on each collection \n",
    "- Validate the collection streams after query worker and CRUD operation execution with expected output\n",
    "- Create streams\n",
    "- Create the stream subscribers\n",
    "- Create the stream publisher. Publisher is pushing some records in the streams\n",
    "- Verify the stream output by counting the number of object received in the stream.\n",
    "- Unpublish the stream worker\n",
    "- Delete all the queryworkers, all the graphs, all the collections, all the streams, all the stream workers, and delete the number of fabric we created in the starting of program\n",
    "\n",
    "\n",
    "If you want to customize the CI/CD pipeline then please comment the appropriate steps and modify the CI/CD pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650d6f6-5ede-429f-abdb-c5938fab21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will execute GDN pipeline in series manner\n",
    "def execute_cicd_pipeline():\n",
    "\n",
    "    log(\"\\n ------- CREATE FABRICS ------\")\n",
    "    create_new_fabrics()\n",
    "    log(\"\\n ------- CREATED FABRICS ------\")\n",
    "\n",
    "    use_fabric()\n",
    "\n",
    "    log(\"\\n ------- CREATE GEO-REPLICATED COLLECTION  ------\")\n",
    "    create_collections()\n",
    "    log(\"\\n ------- CREATED GEO-REPLICATED COLLECTION  ------\")\n",
    "\n",
    "    log(\"\\n ------- CREATE GRAPHS ------\")\n",
    "    create_graphs()\n",
    "    log(\"\\n ------- CREATED GRAPHS  ------\")\n",
    "\n",
    "    log(\"\\n ------- CREATE QUERY WORKER  ------\")\n",
    "    create_query_workers()\n",
    "    log(\"\\n ------- CREATED QUERY WORKER  ------\")\n",
    "\n",
    "    log(\"\\n ------- CREATE STREAM WORKERS  ------\")\n",
    "    create_stream_workers()\n",
    "    log(\"\\n ------- CREATED STREAM WORKERS  ------\")\n",
    "\n",
    "    log(\"\\n ------- PUBLISH STREAM WORKERS  ------\")\n",
    "    activate_stream_workers()\n",
    "    log(\"\\n ------- PUBLISH STREAM WORKERS  ------\")\n",
    "    \n",
    "    log(\"\\n ------- CREATE COLLECTION SUBSCRIBER  ------\")\n",
    "    create_collections_subscribers()\n",
    "    log(\"\\n ------- CREATED COLLECTION SUBSCRIBER  ------\")\n",
    "\n",
    "    log(\"\\n ------- EXECUTE CRUD USING QUERY WORKER ------\")\n",
    "    execute_query_workers()\n",
    "    log(\"\\n ------- EXECUTED CRUD USING QUERY WORKER ------\")\n",
    "\n",
    "    log(\"\\n ------- VERIFY COLLECTION STREAM ------\")\n",
    "    verify_collection_stream()\n",
    "    log(\"\\n ------- VERIFIED COLLECTION STREAM ------\")\n",
    "\n",
    "    log(\"\\n ------- CREATE STREAM ------\")\n",
    "    create_streams()\n",
    "    log(\"\\n ------- CREATED STREAM ------\")\n",
    "\n",
    "    log(\"\\n ------- SUBSCRIBE STREAM ------\")\n",
    "    create_stream_subscribers()\n",
    "    log(\"\\n ------- SUBSCRIBED STREAM ------\")\n",
    "    \n",
    "    log(\"\\n ------- CREATE STREAM PRODUCER ------\")\n",
    "    create_stream_publisher()\n",
    "    log(\"\\n ------- CREATED STREAM PRODUCER ------\")\n",
    "\n",
    "    log(\"\\n ------- VERIFY STREAM ------\")\n",
    "    verify_streams()\n",
    "    log(\"\\n ------- VERIFIED STREAM ------\")\n",
    "\n",
    "    log(\"\\n ------- UNPUBLISH STREAM WORKERS  ------\")\n",
    "    activate_stream_workers(False)\n",
    "    log(\"\\n ------- UNPUBLISH STREAM WORKERS  ------\")\n",
    "\n",
    "    # clearing everything from previous runs\n",
    "    log(\"\\n ------- CLEARING ------\")\n",
    "    clear()\n",
    "    log(\"\\n ------- CLEARED  ------\")\n",
    "\n",
    "    log(json.dumps(info, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04554fa",
   "metadata": {},
   "source": [
    "## 5. Execute CI/CD script\n",
    "\n",
    "Below function is executing the CI/CD pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a405ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the CI/CD pipeline\n",
    "execute_cicd_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba04413",
   "metadata": {},
   "source": [
    "## Section Completed!\n",
    "\n",
    "Congratulations! you have completed this tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
